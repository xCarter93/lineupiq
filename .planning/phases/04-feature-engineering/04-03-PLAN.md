---
phase: 04-feature-engineering
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified: [packages/backend/src/lineupiq/features/pipeline.py, packages/backend/tests/test_feature_pipeline.py, packages/backend/src/lineupiq/features/__init__.py]
autonomous: true
---

<objective>
Create unified feature engineering pipeline that combines all features into ML-ready dataset.

Purpose: Provide a single entry point that orchestrates rolling stats, opponent strength, and weather features into a complete feature set ready for model training.
Output: pipeline.py module with build_features() as the main API for ML-ready data.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plans in this phase
@.planning/phases/04-feature-engineering/04-01-PLAN.md
@.planning/phases/04-feature-engineering/04-02-PLAN.md

# Existing data module for reference
@packages/backend/src/lineupiq/data/__init__.py
@packages/backend/src/lineupiq/data/processing.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create feature pipeline orchestrator</name>
  <files>packages/backend/src/lineupiq/features/pipeline.py, packages/backend/src/lineupiq/features/__init__.py</files>
  <action>
Create pipeline.py module with:

1. `build_features(seasons: list[int], rolling_window: int = 3) -> pl.DataFrame`
   - Main entry point for ML-ready feature data
   - Pipeline:
     a. Load processed data via process_player_stats(seasons)
     b. Add rolling stats via compute_rolling_stats(df, rolling_window)
     c. Add opponent strength via add_opponent_strength(df)
     d. Weather features already included from process_player_stats (temp_normalized, wind_normalized, is_dome)
   - Return complete feature DataFrame

2. `get_feature_columns() -> list[str]`
   - Return list of all feature column names for ML
   - Categorize: rolling features, opponent features, weather features, context features
   - Useful for model training to know which columns are features vs. identifiers

3. `get_target_columns() -> dict[str, list[str]]`
   - Return dict mapping position to target columns
   - QB: ["passing_yards", "passing_tds", "interceptions"]
   - RB: ["rushing_yards", "rushing_tds", "carries", "receiving_yards", "receptions"]
   - WR/TE: ["receiving_yards", "receiving_tds", "receptions"]
   - Used by model training to know what to predict

4. `save_features(df: pl.DataFrame, name: str = "features") -> Path`
   - Save feature DataFrame to data/features/ as Parquet
   - Create directory if needed
   - Return path to saved file

5. Add logging throughout pipeline steps

Update features/__init__.py to export:
- build_features (main API)
- get_feature_columns
- get_target_columns
- save_features
- compute_rolling_stats (from rolling_stats.py)
- add_opponent_strength (from opponent_features.py)
  </action>
  <verify>
```bash
cd packages/backend && uv run python -c "
from lineupiq.features import build_features, get_feature_columns, get_target_columns
import polars as pl

# Build features for 2024
df = build_features([2024])

# Verify all expected feature types present
feature_cols = get_feature_columns()
print(f'Feature columns: {len(feature_cols)}')

# Verify rolling features
rolling_cols = [c for c in df.columns if '_roll' in c]
print(f'Rolling columns: {rolling_cols}')
assert len(rolling_cols) >= 6, 'Should have rolling columns for passing/rushing/receiving'

# Verify opponent features
opp_cols = [c for c in df.columns if 'opp_' in c]
print(f'Opponent columns: {opp_cols}')
assert len(opp_cols) >= 2, 'Should have opponent strength columns'

# Verify weather features (from processing)
assert 'temp_normalized' in df.columns
assert 'wind_normalized' in df.columns

# Verify target columns
targets = get_target_columns()
print(f'Target columns by position: {targets}')
assert 'QB' in targets
assert 'RB' in targets

print(f'Total: {len(df)} rows, {len(df.columns)} columns')
"
```
  </verify>
  <done>
- pipeline.py exists with build_features as main entry point
- Pipeline combines rolling stats + opponent strength + weather
- get_feature_columns and get_target_columns provide ML metadata
- Features module exports all public functions
- Verification script runs without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Add integration tests for feature pipeline</name>
  <files>packages/backend/tests/test_feature_pipeline.py</files>
  <action>
Create test_feature_pipeline.py with:

1. `test_build_features_columns()` - Verify all expected columns present
   - Rolling columns (passing_yards_roll3, etc.)
   - Opponent columns (opp_pass_defense_strength, etc.)
   - Weather columns (temp_normalized, wind_normalized)
   - Context columns (is_home, opponent)

2. `test_build_features_no_nulls_in_features()` - Verify key feature columns don't have nulls
   - Rolling features should have values (min_periods=1)
   - Weather features default to 0 for nulls

3. `test_get_feature_columns()` - Verify returns correct column list
   - Should include all feature types
   - Should NOT include identifier columns (player_id, player_name)

4. `test_get_target_columns()` - Verify position-specific targets
   - QB targets include passing stats
   - RB targets include rushing stats
   - WR/TE targets include receiving stats

5. `test_build_features_with_window()` - Verify custom rolling window works
   - Pass rolling_window=5
   - Verify columns named with _roll5

6. `test_save_and_load_features()` - Verify save/load roundtrip
   - Build features, save, load, compare

Use pytest fixtures to minimize actual data loading (can mock or use minimal season).
  </action>
  <verify>
```bash
cd packages/backend && uv run pytest tests/test_feature_pipeline.py -v
```
  </verify>
  <done>
- test_feature_pipeline.py exists with 6+ tests
- All tests pass
- Tests verify pipeline combines all feature types correctly
- Tests verify feature/target column helpers
  </done>
</task>

<task type="auto">
  <name>Task 3: Run all feature tests and verify integration</name>
  <files></files>
  <action>
Run complete test suite for features module:

1. Run all feature tests together
2. Verify no import errors across modules
3. Verify features module has clean public API

This is a verification task - no new code, just ensuring everything integrates correctly.
  </action>
  <verify>
```bash
cd packages/backend && uv run pytest tests/test_rolling_stats.py tests/test_opponent_features.py tests/test_feature_pipeline.py -v --tb=short
```
  </verify>
  <done>
- All feature tests pass (rolling, opponent, pipeline)
- No import errors
- Features module ready for Phase 5 model development
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd packages/backend && uv run pytest tests/test_feature_pipeline.py -v` passes all tests
- [ ] `cd packages/backend && uv run pytest tests/test_rolling_stats.py tests/test_opponent_features.py tests/test_feature_pipeline.py -v` all pass
- [ ] `cd packages/backend && uv run python -c "from lineupiq.features import build_features, get_feature_columns, get_target_columns"` imports successfully
- [ ] build_features returns DataFrame with rolling + opponent + weather columns
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Feature pipeline combines all feature types
- Clean public API exported from features module
- All feature tests pass
- Ready for Phase 5 model development
</success_criteria>

<output>
After completion, create `.planning/phases/04-feature-engineering/04-03-SUMMARY.md`
</output>
