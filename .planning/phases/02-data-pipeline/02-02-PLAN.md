---
phase: 02-data-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - packages/backend/src/lineupiq/data/storage.py
  - packages/backend/src/lineupiq/data/__init__.py
  - packages/backend/data/.gitkeep
autonomous: true
---

<objective>
Create data storage layer with Parquet persistence and file-based caching for NFL data.

Purpose: Enable efficient storage and retrieval of fetched data, avoiding repeated network calls.
Output: Storage module with cache-aware data loading functions.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-pipeline/DISCOVERY.md
@.planning/phases/02-data-pipeline/02-01-SUMMARY.md
@packages/backend/src/lineupiq/data/fetchers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create data directory structure</name>
  <files>packages/backend/data/.gitkeep, packages/backend/.gitignore</files>
  <action>
1. Create data directory for raw data storage:
   ```
   packages/backend/data/
     raw/
       player_stats/
       schedules/
       snap_counts/
   ```

2. Add `.gitkeep` files to preserve directory structure in git

3. Update `.gitignore` to exclude Parquet files but keep directory structure:
   ```
   # Data files (large, regeneratable)
   data/raw/**/*.parquet
   !data/raw/*/.gitkeep
   ```

This keeps the directory structure versioned but excludes large data files.
  </action>
  <verify>ls -la packages/backend/data/raw/</verify>
  <done>Data directory structure exists with .gitkeep files</done>
</task>

<task type="auto">
  <name>Task 2: Create storage module with Parquet persistence</name>
  <files>packages/backend/src/lineupiq/data/storage.py, packages/backend/src/lineupiq/data/__init__.py</files>
  <action>
Create storage.py with cache-aware loading:

```python
import polars as pl
from pathlib import Path
from datetime import datetime, timedelta
from typing import Callable
import logging

logger = logging.getLogger(__name__)

# Default data directory relative to package
DATA_DIR = Path(__file__).parent.parent.parent.parent / "data" / "raw"

def get_cache_path(data_type: str, key: str) -> Path:
    """Get path for cached data file.

    Args:
        data_type: Category (player_stats, schedules, etc.)
        key: Unique identifier (e.g., season year or "all")

    Returns:
        Path to parquet file.
    """
    return DATA_DIR / data_type / f"{key}.parquet"

def is_cache_valid(path: Path, max_age_days: int = 7) -> bool:
    """Check if cached file exists and is fresh.

    Args:
        path: Path to cache file.
        max_age_days: Maximum age before considered stale.

    Returns:
        True if cache exists and is within max_age.
    """
    if not path.exists():
        return False
    mtime = datetime.fromtimestamp(path.stat().st_mtime)
    return datetime.now() - mtime < timedelta(days=max_age_days)

def save_parquet(df: pl.DataFrame, path: Path) -> None:
    """Save DataFrame to Parquet file.

    Args:
        df: Polars DataFrame to save.
        path: Destination path.
    """
    path.parent.mkdir(parents=True, exist_ok=True)
    df.write_parquet(path)
    logger.info(f"Saved {len(df)} rows to {path}")

def load_parquet(path: Path) -> pl.DataFrame:
    """Load DataFrame from Parquet file.

    Args:
        path: Path to parquet file.

    Returns:
        Polars DataFrame.
    """
    df = pl.read_parquet(path)
    logger.info(f"Loaded {len(df)} rows from {path}")
    return df

def load_with_cache(
    data_type: str,
    key: str,
    fetcher: Callable[[], pl.DataFrame],
    max_age_days: int = 7,
    force_refresh: bool = False,
) -> pl.DataFrame:
    """Load data from cache or fetch if stale/missing.

    Args:
        data_type: Category for cache organization.
        key: Unique identifier for this data.
        fetcher: Function to call if cache miss.
        max_age_days: Cache staleness threshold.
        force_refresh: If True, always fetch fresh data.

    Returns:
        Polars DataFrame from cache or fresh fetch.
    """
    cache_path = get_cache_path(data_type, key)

    if not force_refresh and is_cache_valid(cache_path, max_age_days):
        logger.info(f"Cache hit for {data_type}/{key}")
        return load_parquet(cache_path)

    logger.info(f"Cache miss for {data_type}/{key}, fetching...")
    df = fetcher()
    save_parquet(df, cache_path)
    return df
```

Export key functions from __init__.py:
- load_with_cache
- get_cache_path
- DATA_DIR (for tests)
  </action>
  <verify>uv run python -c "from lineupiq.data.storage import load_with_cache, DATA_DIR; print(DATA_DIR)"</verify>
  <done>Storage module with load_with_cache, save_parquet, load_parquet functions</done>
</task>

<task type="auto">
  <name>Task 3: Create convenience functions for cached data loading</name>
  <files>packages/backend/src/lineupiq/data/storage.py, packages/backend/src/lineupiq/data/__init__.py</files>
  <action>
Add high-level functions that combine fetchers with caching:

```python
from .fetchers import fetch_player_stats, fetch_schedules, fetch_snap_counts

def load_player_stats_cached(
    seasons: list[int],
    max_age_days: int = 7,
    force_refresh: bool = False,
) -> pl.DataFrame:
    """Load player stats with per-season caching.

    Args:
        seasons: List of seasons to load.
        max_age_days: Cache freshness threshold.
        force_refresh: Force re-fetch from nflreadpy.

    Returns:
        Combined DataFrame for all requested seasons.
    """
    dfs = []
    for season in seasons:
        df = load_with_cache(
            data_type="player_stats",
            key=str(season),
            fetcher=lambda s=season: fetch_player_stats([s]),
            max_age_days=max_age_days,
            force_refresh=force_refresh,
        )
        dfs.append(df)
    return pl.concat(dfs) if dfs else pl.DataFrame()

def load_schedules_cached(
    seasons: list[int] | None = None,
    max_age_days: int = 7,
    force_refresh: bool = False,
) -> pl.DataFrame:
    """Load schedules with caching.

    Args:
        seasons: Seasons to load (None = all available).
        max_age_days: Cache freshness threshold.
        force_refresh: Force re-fetch.

    Returns:
        Schedules DataFrame.
    """
    key = "all" if seasons is None else "_".join(map(str, sorted(seasons)))
    return load_with_cache(
        data_type="schedules",
        key=key,
        fetcher=lambda: fetch_schedules(seasons),
        max_age_days=max_age_days,
        force_refresh=force_refresh,
    )
```

Export from __init__.py:
- load_player_stats_cached
- load_schedules_cached
  </action>
  <verify>uv run python -c "from lineupiq.data import load_player_stats_cached; df = load_player_stats_cached([2024]); print(f'Loaded {len(df)} rows')"</verify>
  <done>Convenience functions load_player_stats_cached and load_schedules_cached work and create cache files</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Data directory structure exists: packages/backend/data/raw/{player_stats,schedules,snap_counts}/
- [ ] load_with_cache function works correctly
- [ ] load_player_stats_cached creates .parquet files in data/raw/player_stats/
- [ ] Second call to load_player_stats_cached reads from cache (check logs)
- [ ] .gitignore excludes parquet files but keeps directory
</verification>

<success_criteria>
- All tasks completed
- Data persisted as Parquet files
- Cache hits work correctly (second load is fast)
- Directory structure committed to git (without data)
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-pipeline/02-02-SUMMARY.md`
</output>
