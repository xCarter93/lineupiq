---
phase: 03-data-processing
plan: 03
type: execute
wave: 3
depends_on: ["03-01", "03-02"]
files_modified:
  - packages/backend/src/lineupiq/data/processing.py
  - packages/backend/src/lineupiq/data/__init__.py
  - packages/backend/tests/test_processing.py
autonomous: true
---

<objective>
Create weekly stat aggregation and data processing pipeline.

Purpose: Produce ML-ready weekly player stats by combining cleaning, normalization, and schedule joining. Output is the input for Phase 4 feature engineering.
Output: processing.py module with full data processing pipeline that produces clean, normalized, weekly player stats with game context.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-data-processing/03-01-SUMMARY.md
@.planning/phases/03-data-processing/03-02-SUMMARY.md

@packages/backend/src/lineupiq/data/fetchers.py
@packages/backend/src/lineupiq/data/storage.py
@packages/backend/src/lineupiq/data/cleaning.py
@packages/backend/src/lineupiq/data/normalization.py
@packages/backend/src/lineupiq/data/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create schedule joining and opponent context</name>
  <files>packages/backend/src/lineupiq/data/processing.py</files>
  <action>
Create processing.py module with schedule context functions:

1. `add_game_context(player_df: pl.DataFrame, schedule_df: pl.DataFrame) -> pl.DataFrame`:
   - Join player stats with schedule data on season + week
   - Determine if player is home or away based on recent_team vs home_team/away_team
   - Add columns:
     - is_home: bool (True if player's team == home_team)
     - opponent: str (away_team if home, home_team if away)
     - game_id: from schedule
   - Handle cases where team isn't in game (bye week, etc.) by setting opponent to null
   - Log join stats

2. `add_weather_context(df: pl.DataFrame) -> pl.DataFrame`:
   - Expects schedule columns already joined
   - Add columns:
     - temp_normalized: (temp - 65) / 20 (center around comfortable, scale to ~[-2, 2])
     - wind_normalized: wind / 15 (scale to ~[0, 2])
     - is_dome: from schedule join (already added in clean_schedules)
   - Fill null weather with 0 (neutral conditions)
   - These become features for weather impact on passing

Use Polars join expressions. Handle mismatched seasons gracefully.
  </action>
  <verify>cd packages/backend && uv run python -c "from lineupiq.data.processing import add_game_context, add_weather_context; print('Import OK')"</verify>
  <done>processing.py exists with add_game_context and add_weather_context functions</done>
</task>

<task type="auto">
  <name>Task 2: Create full processing pipeline</name>
  <files>packages/backend/src/lineupiq/data/processing.py</files>
  <action>
Add main processing pipeline function to processing.py:

1. `process_player_stats(seasons: list[int], force_refresh: bool = False) -> pl.DataFrame`:
   Main pipeline orchestrator:

   a. Load raw data using cached functions:
      - player_stats = load_player_stats_cached(seasons, force_refresh=force_refresh)
      - schedules = load_schedules_cached(seasons, force_refresh=force_refresh)

   b. Clean data:
      - player_stats = clean_player_stats(player_stats)
      - schedules = clean_schedules(schedules)

   c. Normalize:
      - player_stats = normalize_player_data(player_stats)
      - schedules = normalize_team_columns(schedules)

   d. Add context:
      - player_stats = add_game_context(player_stats, schedules)
      - player_stats = add_weather_context(player_stats)

   e. Sort by season, week, player_id for consistent ordering

   f. Log pipeline stats: rows in, rows out, columns

   Returns: Fully processed DataFrame ready for feature engineering

2. `save_processed_data(df: pl.DataFrame, name: str = "processed_stats") -> Path`:
   - Save to data/processed/{name}.parquet
   - Create directory if needed
   - Return path for confirmation

This is THE main entry point for getting ML-ready data.
  </action>
  <verify>cd packages/backend && uv run python -c "from lineupiq.data.processing import process_player_stats; print('Import OK')"</verify>
  <done>process_player_stats and save_processed_data functions added</done>
</task>

<task type="auto">
  <name>Task 3: Update exports and add integration tests</name>
  <files>packages/backend/src/lineupiq/data/__init__.py, packages/backend/tests/test_processing.py, packages/backend/data/processed/.gitkeep</files>
  <action>
1. Create data/processed/.gitkeep directory for processed data storage.

2. Update __init__.py to export:
   - add_game_context
   - add_weather_context
   - process_player_stats
   - save_processed_data

3. Create test_processing.py with tests:
   - test_add_game_context_determines_home_away: Create player on KC, game KC vs BUF, verify is_home=True, opponent=BUF
   - test_add_game_context_handles_away: Same but player on BUF, verify is_home=False, opponent=KC
   - test_add_weather_context_normalizes: temp=85 -> temp_normalized=1.0, wind=30 -> wind_normalized=2.0
   - test_add_weather_context_handles_nulls: null temp -> 0 normalized
   - test_process_player_stats_returns_expected_columns: Mock or use small dataset, verify output has is_home, opponent, temp_normalized, wind_normalized

Keep tests focused. Use mock data rather than real API calls for unit tests.
  </action>
  <verify>cd packages/backend && uv run pytest tests/test_processing.py -v</verify>
  <done>Exports added, processed directory created, tests pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd packages/backend && uv run python -c "from lineupiq.data import process_player_stats"` succeeds
- [ ] `cd packages/backend && uv run pytest tests/test_processing.py -v` passes all tests
- [ ] `packages/backend/data/processed/.gitkeep` exists
- [ ] Full pipeline runs: `cd packages/backend && uv run python -c "from lineupiq.data import process_player_stats; df = process_player_stats([2024]); print(f'Processed: {df.shape}, columns: {df.columns[:10]}...')"`
</verification>

<success_criteria>
- All tasks completed
- processing.py module exists with full pipeline
- process_player_stats produces clean, normalized data with game context
- Output includes is_home, opponent, weather normalization
- Tests pass for all processing functions
- Ready for Phase 4 feature engineering
</success_criteria>

<output>
After completion, create `.planning/phases/03-data-processing/03-03-SUMMARY.md`
</output>
