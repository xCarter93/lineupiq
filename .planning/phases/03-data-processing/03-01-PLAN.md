---
phase: 03-data-processing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/backend/src/lineupiq/data/cleaning.py
  - packages/backend/src/lineupiq/data/__init__.py
  - packages/backend/tests/test_cleaning.py
autonomous: true
---

<objective>
Create data cleaning and validation module for raw NFL data.

Purpose: Establish clean data foundation before normalization and aggregation. Remove invalid rows, handle nulls, validate data types, and filter to ML-relevant columns.
Output: cleaning.py module with reusable cleaning functions for player stats and schedules data.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@packages/backend/src/lineupiq/data/fetchers.py
@packages/backend/src/lineupiq/data/storage.py
@packages/backend/src/lineupiq/data/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create cleaning module with core validation functions</name>
  <files>packages/backend/src/lineupiq/data/cleaning.py</files>
  <action>
Create cleaning.py module with Polars-based cleaning functions:

1. `validate_player_stats(df: pl.DataFrame) -> pl.DataFrame`:
   - Remove rows where player_id is null (required identifier)
   - Remove rows where position is null or not in SKILL_POSITIONS
   - Remove rows where season is null
   - Remove rows where week is null or <= 0
   - Log rows removed at each step

2. `clean_numeric_stats(df: pl.DataFrame) -> pl.DataFrame`:
   - Fill null numeric stat columns (passing_yards, rushing_yards, etc.) with 0
   - These columns: passing_yards, passing_tds, interceptions, rushing_yards, rushing_tds, carries, receptions, receiving_yards, receiving_tds, targets
   - Cap obvious outliers: yards > 600 (single game record ~550), TDs > 8 per game
   - Log any capped values

3. `select_ml_columns(df: pl.DataFrame) -> pl.DataFrame`:
   - Select only columns needed for ML:
     - Identifiers: player_id, player_name, player_display_name, position, recent_team, season, week
     - Passing: passing_yards, passing_tds, interceptions, attempts, completions
     - Rushing: rushing_yards, rushing_tds, carries
     - Receiving: receptions, receiving_yards, receiving_tds, targets
     - Fantasy (for validation): fantasy_points, fantasy_points_ppr
   - Return DataFrame with only these columns (ignore missing ones gracefully)

4. `clean_player_stats(df: pl.DataFrame) -> pl.DataFrame`:
   - Orchestrator function that calls: validate_player_stats -> clean_numeric_stats -> select_ml_columns
   - Returns fully cleaned DataFrame ready for normalization

Use logging throughout. Import SKILL_POSITIONS from fetchers module.
  </action>
  <verify>cd packages/backend && uv run python -c "from lineupiq.data.cleaning import clean_player_stats; print('Import OK')"</verify>
  <done>cleaning.py exists with validate_player_stats, clean_numeric_stats, select_ml_columns, and clean_player_stats functions</done>
</task>

<task type="auto">
  <name>Task 2: Add schedule data cleaning function</name>
  <files>packages/backend/src/lineupiq/data/cleaning.py</files>
  <action>
Add schedule cleaning function to cleaning.py:

1. `clean_schedules(df: pl.DataFrame) -> pl.DataFrame`:
   - Remove rows where game_id is null
   - Remove rows where season or week is null
   - Select ML-relevant columns:
     - Game: game_id, season, week, game_type, gameday
     - Teams: home_team, away_team, home_score, away_score
     - Weather: temp, wind, roof (dome affects passing)
     - Venue: surface, stadium_id
   - Fill null temp/wind with reasonable defaults (65, 5) for dome games
   - Normalize roof values to boolean is_dome (True if 'dome' or 'closed')
   - Log cleaning stats

This enables joining player stats with game conditions for feature engineering.
  </action>
  <verify>cd packages/backend && uv run python -c "from lineupiq.data.cleaning import clean_schedules; print('Import OK')"</verify>
  <done>clean_schedules function added to cleaning.py</done>
</task>

<task type="auto">
  <name>Task 3: Update exports and add basic tests</name>
  <files>packages/backend/src/lineupiq/data/__init__.py, packages/backend/tests/test_cleaning.py</files>
  <action>
1. Update __init__.py to export:
   - clean_player_stats
   - clean_schedules
   - validate_player_stats (for granular use)
   - clean_numeric_stats (for granular use)
   - select_ml_columns (for granular use)

2. Create test_cleaning.py with tests:
   - test_validate_player_stats_removes_null_player_id: Create df with null player_id, verify removed
   - test_clean_numeric_stats_fills_nulls: Verify null passing_yards becomes 0
   - test_clean_numeric_stats_caps_outliers: Verify 700 yards capped to 600
   - test_select_ml_columns_filters: Verify only expected columns returned
   - test_clean_player_stats_full_pipeline: Create messy df, verify clean output
   - test_clean_schedules_adds_is_dome: Verify dome roof -> is_dome=True

Use Polars for test DataFrames. Keep tests focused on behavior.
  </action>
  <verify>cd packages/backend && uv run pytest tests/test_cleaning.py -v</verify>
  <done>All exports added to __init__.py and tests pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd packages/backend && uv run python -c "from lineupiq.data import clean_player_stats, clean_schedules"` succeeds
- [ ] `cd packages/backend && uv run pytest tests/test_cleaning.py -v` passes all tests
- [ ] `cd packages/backend && uv run python -c "from lineupiq.data import load_player_stats_cached; from lineupiq.data.cleaning import clean_player_stats; df = load_player_stats_cached([2024]); cleaned = clean_player_stats(df); print(f'Cleaned: {cleaned.shape}')"` shows reduced column count
</verification>

<success_criteria>
- All tasks completed
- cleaning.py module exists with documented functions
- All cleaning functions work with Polars DataFrames
- Exports updated in __init__.py
- Tests pass for all cleaning functions
</success_criteria>

<output>
After completion, create `.planning/phases/03-data-processing/03-01-SUMMARY.md`
</output>
