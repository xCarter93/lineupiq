---
phase: 07-prediction-api
plan: 03
type: execute
wave: 3
depends_on: ["07-02"]
files_modified:
  - packages/backend/src/lineupiq/api/cache.py
  - packages/backend/src/lineupiq/api/routes/predictions.py
  - packages/backend/src/lineupiq/api/main.py
  - packages/backend/tests/test_cache.py
autonomous: true
---

<objective>
Add in-memory response caching for prediction endpoints to reduce redundant model inference.

Purpose: Cache prediction results keyed by feature hash so identical requests return cached responses. Improves response time for repeated predictions (same player/matchup scenarios).
Output: Prediction endpoints return cached responses for duplicate requests with cache hit headers.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-prediction-api/07-02-SUMMARY.md

# Existing routes to update
@packages/backend/src/lineupiq/api/routes/predictions.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create cache module</name>
  <files>packages/backend/src/lineupiq/api/cache.py</files>
  <action>
Create cache.py with simple in-memory caching:

1. PredictionCache class:
   - __init__(max_size: int = 1000, ttl_seconds: int = 3600)
   - Store dict[str, tuple[Any, float]] mapping cache_key to (response, timestamp)
   - max_size limits entries (LRU eviction when full)
   - ttl_seconds for entry expiration

2. Methods:
   - _make_key(position: str, features: dict) -> str:
     Create deterministic hash from position + sorted feature values
     Use hashlib.sha256 for consistent hashing

   - get(position: str, features: dict) -> Any | None:
     Return cached response if exists and not expired
     Return None on miss or expiration

   - set(position: str, features: dict, response: Any) -> None:
     Store response with current timestamp
     Evict oldest entries if over max_size

   - clear() -> None:
     Empty the cache

   - stats() -> dict:
     Return {"size": int, "max_size": int, "hits": int, "misses": int}

Track hits/misses for observability. Thread-safe not required (single-threaded uvicorn default).
  </action>
  <verify>
cd packages/backend && uv run python -c "from lineupiq.api.cache import PredictionCache; c = PredictionCache(); print('Cache OK')"
  </verify>
  <done>PredictionCache class importable with all methods</done>
</task>

<task type="auto">
  <name>Task 2: Integrate cache into prediction routes</name>
  <files>packages/backend/src/lineupiq/api/routes/predictions.py, packages/backend/src/lineupiq/api/main.py</files>
  <action>
1. Update main.py:
   - Import PredictionCache
   - Create cache instance in lifespan: app.state.cache = PredictionCache()
   - Add GET /cache/stats endpoint returning app.state.cache.stats()
   - Add DELETE /cache endpoint to clear cache (returns {"cleared": True})

2. Update predictions.py routes to use cache:

For each prediction endpoint (qb, rb, wr, te):
   - At start: check cache.get(position, features_dict)
   - If hit: return cached response immediately, add X-Cache: HIT header
   - If miss: run prediction, cache.set(position, features_dict, response), add X-Cache: MISS header

Use fastapi.Response to add custom headers.
Convert PredictionRequest to dict via request.model_dump() for cache key.

Keep existing prediction logic unchanged - cache wraps it.
  </action>
  <verify>
cd packages/backend && uv run python -c "from lineupiq.api.main import app; print('Endpoints:', len(app.routes))"
  </verify>
  <done>Cache integrated with prediction routes, /cache/stats endpoint exists</done>
</task>

<task type="auto">
  <name>Task 3: Add cache tests</name>
  <files>packages/backend/tests/test_cache.py</files>
  <action>
Create test_cache.py with:

Unit tests for PredictionCache:
  - test_cache_set_get(): Set value, get returns it
  - test_cache_miss(): Get non-existent key returns None
  - test_cache_expiration(): Set with ttl, wait, verify expired
  - test_cache_max_size(): Fill past max_size, verify old entries evicted
  - test_cache_stats(): Verify hits/misses count correctly

Integration tests with TestClient:
  - test_prediction_caching():
    POST /predict/qb with same features twice
    First call: verify X-Cache: MISS
    Second call: verify X-Cache: HIT
    Verify responses are identical

  - test_cache_stats_endpoint():
    GET /cache/stats
    Verify returns size, hits, misses keys

  - test_cache_clear_endpoint():
    POST predictions, DELETE /cache, verify stats reset

Use pytest fixtures for cache instance and test client.
For expiration test, use small ttl (0.1s) and time.sleep(0.2).
  </action>
  <verify>cd packages/backend && uv run pytest tests/test_cache.py -v</verify>
  <done>All cache tests pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `curl localhost:8000/cache/stats` returns cache statistics
- [ ] POST same prediction twice: first X-Cache: MISS, second X-Cache: HIT
- [ ] `curl -X DELETE localhost:8000/cache` clears cache
- [ ] `cd packages/backend && uv run pytest tests/test_cache.py -v` passes
- [ ] `cd packages/backend && uv run pytest tests/ -v` all tests still pass
</verification>

<success_criteria>

- In-memory cache with TTL and max size limits
- Prediction endpoints check cache before inference
- X-Cache header indicates HIT or MISS
- /cache/stats endpoint for observability
- /cache DELETE endpoint for manual clearing
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-prediction-api/07-03-SUMMARY.md`
</output>
