---
phase: 06-model-evaluation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified: [packages/backend/src/lineupiq/models/importance.py, packages/backend/tests/test_importance.py, packages/backend/src/lineupiq/models/__init__.py]
autonomous: true
---

<objective>
Create feature importance analysis module using SHAP values and XGBoost native importance.

Purpose: Understand which features contribute most to predictions, enabling feature selection and model interpretation. PROJECT.md emphasizes feature importance analysis to avoid overfitting from previous attempts.
Output: importance.py with SHAP-based analysis and XGBoost feature importance extraction.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 5 established patterns
@.planning/phases/05-model-development/05-01-SUMMARY.md

# Source files
@packages/backend/src/lineupiq/models/persistence.py
@packages/backend/src/lineupiq/features/pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create feature importance module</name>
  <files>packages/backend/src/lineupiq/models/importance.py</files>
  <action>
Create importance.py with:

1. `get_xgb_importance(model, feature_names=None) -> dict[str, float]`
   - Extract XGBoost native feature importance (gain-based)
   - Use model.get_booster().get_score(importance_type="gain")
   - Return dict mapping feature name -> importance value
   - Normalize to sum to 1.0 for comparability

2. `compute_shap_values(model, X, feature_names=None) -> tuple[np.ndarray, np.ndarray]`
   - Create SHAP TreeExplainer for XGBoost model
   - Compute SHAP values for samples in X
   - Return (shap_values, expected_value) tuple
   - Note: SHAP values shape is (n_samples, n_features)

3. `get_shap_importance(shap_values, feature_names) -> dict[str, float]`
   - Compute mean absolute SHAP values per feature
   - Return dict mapping feature name -> mean |SHAP| value
   - Normalize to sum to 1.0

4. `analyze_feature_importance(position, target, X_sample=None, n_samples=100) -> dict`
   - Load model via load_model(position, target)
   - Get XGBoost native importance
   - If X_sample provided, also compute SHAP importance
   - Return dict with:
     - xgb_importance: dict of feature -> importance
     - shap_importance: dict of feature -> importance (if X_sample provided)
     - top_features: list of top 5 features by XGBoost importance
     - position, target metadata

Use shap.TreeExplainer which is optimized for XGBoost models.
Handle case where X_sample is not provided (skip SHAP, return XGBoost importance only).
  </action>
  <verify>uv run python -c "from lineupiq.models.importance import get_xgb_importance, analyze_feature_importance; print('Imports OK')"</verify>
  <done>importance.py exists with all 4 functions importable</done>
</task>

<task type="auto">
  <name>Task 2: Add exports to models __init__.py</name>
  <files>packages/backend/src/lineupiq/models/__init__.py</files>
  <action>
Add importance module exports to __init__.py:
- get_xgb_importance
- compute_shap_values
- get_shap_importance
- analyze_feature_importance

Follow existing pattern for other module exports.
  </action>
  <verify>uv run python -c "from lineupiq.models import analyze_feature_importance; print('Exports OK')"</verify>
  <done>Importance functions accessible from lineupiq.models</done>
</task>

<task type="auto">
  <name>Task 3: Add feature importance tests</name>
  <files>packages/backend/tests/test_importance.py</files>
  <action>
Create test file with:

1. test_get_xgb_importance_returns_dict - Verify XGBoost importance extraction
2. test_xgb_importance_normalized - Verify values sum to 1.0
3. test_compute_shap_values_shape - Verify SHAP values have correct shape
4. test_get_shap_importance_returns_dict - Verify SHAP importance aggregation
5. test_analyze_feature_importance_without_samples - Test XGBoost-only mode
6. test_analyze_feature_importance_with_samples - Test full SHAP analysis

Use small synthetic data for fast tests. Test with actual QB model if available.
SHAP tests may need @pytest.mark.slow decorator if they take >1s.
  </action>
  <verify>cd packages/backend && uv run pytest tests/test_importance.py -v</verify>
  <done>All importance tests pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `uv run python -c "from lineupiq.models.importance import *"` succeeds
- [ ] `cd packages/backend && uv run pytest tests/test_importance.py -v` passes
- [ ] `cd packages/backend && uv run pytest --tb=short` passes (no regressions)
</verification>

<success_criteria>

- All tasks completed
- XGBoost native importance extraction works
- SHAP values computed correctly for TreeExplainer
- analyze_feature_importance returns both importance types when sample data provided
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-model-evaluation/06-02-SUMMARY.md`
</output>
