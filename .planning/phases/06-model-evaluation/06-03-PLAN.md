---
phase: 06-model-evaluation
plan: 03
type: execute
wave: 1
depends_on: []
files_modified: [packages/backend/src/lineupiq/models/diagnostics.py, packages/backend/tests/test_diagnostics.py, packages/backend/src/lineupiq/models/__init__.py]
autonomous: true
---

<objective>
Create overfitting detection module comparing training vs holdout performance to identify models that may need correction.

Purpose: Detect overfitting by comparing train/test performance gaps. PROJECT.md notes previous attempts failed due to overfitting - this module provides quantitative overfitting metrics.
Output: diagnostics.py with training/test comparison and overfitting detection functions.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 5 established patterns
@.planning/phases/05-model-development/05-01-SUMMARY.md

# Source files
@packages/backend/src/lineupiq/models/persistence.py
@packages/backend/src/lineupiq/models/training.py
@packages/backend/src/lineupiq/features/pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create diagnostics module</name>
  <files>packages/backend/src/lineupiq/models/diagnostics.py</files>
  <action>
Create diagnostics.py with:

1. `compute_train_metrics(model, X_train, y_train) -> dict`
   - Generate predictions on training data
   - Compute MAE, RMSE, R² on training set
   - Return dict with train_mae, train_rmse, train_r2

2. `compute_overfit_ratio(train_rmse, test_rmse) -> float`
   - Calculate overfit ratio: test_rmse / train_rmse
   - Ratio close to 1.0 = good generalization
   - Ratio >> 1.0 = overfitting (test much worse than train)
   - Return the ratio value

3. `diagnose_overfitting(train_rmse, test_rmse, threshold=1.3) -> dict`
   - Compute overfit ratio
   - Flag as "overfitting" if ratio > threshold
   - Flag as "underfitting" if train_rmse is very high AND ratio close to 1.0
   - Flag as "healthy" otherwise
   - Return dict with: ratio, status, recommendation

4. `run_diagnostics(position, target, train_df, test_df, feature_cols) -> dict`
   - Load model via load_model(position, target)
   - Filter dataframes to position
   - Prepare X_train, y_train, X_test, y_test
   - Compute train metrics
   - Compute test metrics
   - Compute overfit ratio and diagnosis
   - Return comprehensive dict with:
     - position, target
     - train_metrics: dict
     - test_metrics: dict
     - overfit_ratio: float
     - diagnosis: dict (status, recommendation)
     - n_train, n_test sample counts

5. `run_all_diagnostics(train_df, test_df) -> list[dict]`
   - Get all models via list_models()
   - Run diagnostics for each
   - Return list of results

Use sklearn.metrics for MAE/RMSE/R². Import from lineupiq.models.persistence.
Threshold 1.3 means test RMSE up to 30% higher than train is acceptable.
  </action>
  <verify>uv run python -c "from lineupiq.models.diagnostics import run_diagnostics, diagnose_overfitting; print('Imports OK')"</verify>
  <done>diagnostics.py exists with all 5 functions importable</done>
</task>

<task type="auto">
  <name>Task 2: Add exports to models __init__.py</name>
  <files>packages/backend/src/lineupiq/models/__init__.py</files>
  <action>
Add diagnostics module exports to __init__.py:
- compute_train_metrics
- compute_overfit_ratio
- diagnose_overfitting
- run_diagnostics
- run_all_diagnostics

Follow existing pattern for other module exports.
  </action>
  <verify>uv run python -c "from lineupiq.models import run_diagnostics, run_all_diagnostics; print('Exports OK')"</verify>
  <done>Diagnostics functions accessible from lineupiq.models</done>
</task>

<task type="auto">
  <name>Task 3: Add diagnostics tests</name>
  <files>packages/backend/tests/test_diagnostics.py</files>
  <action>
Create test file with:

1. test_compute_overfit_ratio_healthy - ratio ~1.0 for balanced train/test
2. test_compute_overfit_ratio_overfitting - ratio > 1.3 when test >> train
3. test_diagnose_overfitting_healthy - returns status="healthy" for good models
4. test_diagnose_overfitting_flags_overfit - returns status="overfitting" when ratio > threshold
5. test_run_diagnostics_returns_complete_dict - verify all expected keys present
6. test_run_diagnostics_with_actual_model - test with QB model if available

Use synthetic data where appropriate for deterministic tests.
Ensure tests cover edge cases: perfect model (ratio=1.0), severely overfit (ratio>2.0).
  </action>
  <verify>cd packages/backend && uv run pytest tests/test_diagnostics.py -v</verify>
  <done>All diagnostics tests pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `uv run python -c "from lineupiq.models.diagnostics import *"` succeeds
- [ ] `cd packages/backend && uv run pytest tests/test_diagnostics.py -v` passes
- [ ] `cd packages/backend && uv run pytest --tb=short` passes (no regressions)
</verification>

<success_criteria>

- All tasks completed
- Overfit ratio calculation works correctly
- Diagnosis function properly flags overfitting vs healthy models
- run_diagnostics provides complete train/test comparison
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-model-evaluation/06-03-SUMMARY.md`
</output>
