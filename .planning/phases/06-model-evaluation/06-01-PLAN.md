---
phase: 06-model-evaluation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [packages/backend/src/lineupiq/models/evaluation.py, packages/backend/tests/test_evaluation.py, packages/backend/src/lineupiq/models/__init__.py]
autonomous: true
---

<objective>
Create model evaluation module with comprehensive metrics (MAE, RMSE, R²) and holdout validation for all 13 trained models.

Purpose: Provide quantitative model performance assessment beyond CV scores stored during training. Generate holdout test results using data the models haven't seen.
Output: Evaluation module with evaluate_model, evaluate_all_models, and create_holdout_split functions.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 5 established patterns
@.planning/phases/05-model-development/05-01-SUMMARY.md

# Source files
@packages/backend/src/lineupiq/models/persistence.py
@packages/backend/src/lineupiq/models/training.py
@packages/backend/src/lineupiq/features/pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create evaluation module with metrics</name>
  <files>packages/backend/src/lineupiq/models/evaluation.py</files>
  <action>
Create evaluation.py with:

1. `calculate_metrics(y_true, y_pred) -> dict` - Returns dict with:
   - mae: Mean Absolute Error
   - rmse: Root Mean Squared Error
   - r2: R-squared coefficient
   - mape: Mean Absolute Percentage Error (handle zeros with np.where)

2. `create_holdout_split(df, test_season=2024) -> tuple[pl.DataFrame, pl.DataFrame]`
   - Split data by season: train on seasons < test_season, test on test_season
   - Return (train_df, test_df) tuple
   - This provides true out-of-sample validation (model trained on 2019-2023, tested on 2024)

3. `evaluate_model(position, target, test_df) -> dict`
   - Load model via load_model(position, target)
   - Get feature columns via get_feature_columns()
   - Filter test_df to position
   - Prepare X (features) and y (target) arrays
   - Generate predictions
   - Return metrics dict plus n_samples, position, target

4. `evaluate_all_models(test_df) -> list[dict]`
   - Call list_models() to get all trained models
   - Call evaluate_model for each
   - Return list of results dicts

Use sklearn.metrics for MAE, RMSE (mean_squared_error with squared=False), R2.
Import from lineupiq.models.persistence and lineupiq.features.pipeline.
  </action>
  <verify>uv run python -c "from lineupiq.models.evaluation import calculate_metrics, evaluate_model, evaluate_all_models; print('Imports OK')"</verify>
  <done>evaluation.py exists with all 4 functions importable</done>
</task>

<task type="auto">
  <name>Task 2: Add exports to models __init__.py</name>
  <files>packages/backend/src/lineupiq/models/__init__.py</files>
  <action>
Add evaluation module exports to __init__.py:
- calculate_metrics
- create_holdout_split
- evaluate_model
- evaluate_all_models

Follow existing pattern for other module exports.
  </action>
  <verify>uv run python -c "from lineupiq.models import evaluate_model, evaluate_all_models; print('Exports OK')"</verify>
  <done>Evaluation functions accessible from lineupiq.models</done>
</task>

<task type="auto">
  <name>Task 3: Add evaluation tests</name>
  <files>packages/backend/tests/test_evaluation.py</files>
  <action>
Create test file with:

1. test_calculate_metrics_accuracy - Verify metrics calculation on known values
2. test_calculate_metrics_handles_zeros - Verify MAPE handles zero targets
3. test_create_holdout_split - Verify season-based splitting
4. test_evaluate_model_returns_metrics - Test with one model (QB_passing_yards)
5. test_evaluate_all_models_returns_list - Verify all models evaluated

Use pytest fixtures where appropriate. Mock or use actual models if available.
Tests should be deterministic and not require full model training.
  </action>
  <verify>cd packages/backend && uv run pytest tests/test_evaluation.py -v</verify>
  <done>All evaluation tests pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `uv run python -c "from lineupiq.models.evaluation import *"` succeeds
- [ ] `cd packages/backend && uv run pytest tests/test_evaluation.py -v` passes
- [ ] `cd packages/backend && uv run pytest --tb=short` passes (no regressions)
</verification>

<success_criteria>

- All tasks completed
- Evaluation module provides MAE, RMSE, R², MAPE metrics
- Holdout split function creates proper train/test separation by season
- evaluate_all_models can assess all 13 trained models
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-model-evaluation/06-01-SUMMARY.md`
</output>
